逻辑回归算法梳理
=============

1. [逻辑回归与线性回归的联系与区别](#user-content-逻辑回归与线性回归的联系与区别)
2. [逻辑回归的原理](#user-content-逻辑回归的原理)
3. [逻辑回归损失函数推导及优化](#user-content-逻辑回归损失函数推导及优化)
4. [正则化与模型评估指标](#user-content-正则化与模型评估指标)
5. [逻辑回归的优缺点](#user-content-逻辑回归的优缺点)
6. [样本不均衡问题解决办法](#user-content-样本不均衡问题解决办法)
7. [sklearn参数](#user-content-sklearn参数)

# 逻辑回归与线性回归的联系与区别

## 定义
~~~
简单来说， 逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。注意，这里用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。
~~~

## 与线性回归的联系与区别
~~~
逻辑回归（Logistic Regression）与线性回归（Linear Regression）都是一种广义线性模型（generalized linear model）。逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。 因此与线性回归有很多相同之处，去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。可以说，逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。
~~~

## 个人理解
~~~
线程回归处理连续值，逻辑回归处理散点值，适合分类。问题：预测最终是一个概率值，且包含0和1的概率，回归到预测是如何联系起来的？需要后续学习。
~~~

**参考**
- https://zh.wikipedia.org/wiki/%E9%82%8F%E8%BC%AF%E8%BF%B4%E6%AD%B8
- https://baike.baidu.com/item/logistic%E5%9B%9E%E5%BD%92
- https://zhuanlan.zhihu.com/p/28408516

# 逻辑回归的原理
~~~
Logistic 回归通过使用其固有的 logistic 函数估计概率，来衡量因变量（我们想要预测的标签）与一个或多个自变量（特征）之间的关系。

然后这些概率必须二值化才能真地进行预测。这就是 logistic 函数的任务，也称为 Sigmoid 函数。Sigmoid 函数是一个 S 形曲线，它可以将任意实数值映射到介于 0 和 1 之间的值，但并不能取到 0或1。然后使用阈值分类器将 0 和 1 之间的值转换为 0 或 1。
~~~

**参考**
- http://bluewhale.cc/2016-05-18/logistic-regression.html
- https://www.cnblogs.com/hust-ghtao/p/3683933.html
- http://blog.sciencenet.cn/blog-520608-745856.html

# 逻辑回归损失函数推导及优化
~~~
推导暂时还看不懂，先记录。 // TODO 再捡起高数
~~~

**参考**
- https://www.cnblogs.com/pinard/p/6029432.html
- http://bluewhale.cc/2016-05-18/logistic-regression.html
- https://www.cnblogs.com/hust-ghtao/p/3683933.html

# 正则化与模型评估指标

## 正则化
~~~
解决过拟合问题。
保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。
当我们有很多特征时，这个方法非常有效;
模型选择的典型方法是正则化。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大；
正则化的作用是选择经验风险最小和模型复杂度同时较小的模型；从贝叶斯估计的角度来看，正则化项对应于模型的先验概率；
~~~

**参考**
- https://blog.csdn.net/whuhan2013/article/details/53454811
- https://blog.csdn.net/u012328159/article/details/51093922
- https://www.cnblogs.com/dfcao/p/ng_ml_3.html
- https://www.cnblogs.com/jianxinzhou/p/4083921.html
- https://segmentfault.com/a/1190000014807779
- https://www.zhihu.com/question/20700829
- https://tech.meituan.com/2015/05/08/intro-to-logistic-regression.html

## 模型评估指标
v1
~~~
错误率,精度,误差的基本概念:
  错误率(error rate)= a个样本分类错误/m个样本
  精度(accuracy)= 1 -错误率
  误差(error)：学习器实际预测输出与样本的真是输出之间的差异。
  训练误差(training error)：即经验误差。学习器在训练集上的误差。
  泛化误差(generalization error)：学习器在新样本上的误差。

分类器评估指标
对于二分类问题，可将样例根据其真实类别和分类器预测类别划分为：
真正例（True Positive，TP）：真实类别为正例，预测类别为正例。
假正例（False Positive，FP）：真实类别为负例，预测类别为正例。
假负例（False Negative，FN）：真实类别为正例，预测类别为负例。
真负例（True Negative，TN）：真实类别为负例，预测类别为负例。
~~~

**参考**
- https://juejin.im/entry/5b3b4254e51d45194a51e61a

v2
~~~
1. Mean dependent var
因变量的样本均值: 目的是为了度量因变量的集中度
2. S.D dependent var
因变量的样本标准差: 目的是为了度量因变量的离散度
3. sum squared redis
残差平方和：很多最优化的方法都怡残差平方最小和作为目标函数。越小说明效果越好。残差平方和会随着回归方程右边变量的增加而减少。
4. S.E regression
回归标准差：显然是越小越好
5. log likelihood
和残差一样，可以作为最大似然估计的目标函数，越大越好。
6. F statistic
检验回归方程的显著性：自变量和因变量的线性关系是否密切。给定显著水平a, 根据自由度(k,n-k-1)查F分布表，
若F>Fa,则显著，否则不显著。以上说的密切关系指的是所有自变量的联合。也就是说至少有一个变量有关，则显著。
F统计量实际上就是检验当删除所有因变量的时候，残差平方和会增加。
7. Prob(F-Statistic)
F检验对应的概率，越小越好。
8. T statistic
判断回归模型右边每个属性是否与因变量关系密切。
同样T>Ta则拒绝原假设。否则该变量可以剔除。
9. Prob(T-Statistic)
T检验对应的概率，越小越好。
10. R-squared
R方的取值范围位于[0,1]之间：目的是描述预测y的程度，显然是越大越好，但是也不能因为大就完全认为回归效果好，
还要结合其他的参数，因为R方的值可能因为其他非回归预测效果好的原因导致值变大。
11. Adjusted R-squared
目的是为了克服上面所说的因为其他的原因(变量个数增大)导致R方的递增。
12. Durbin-waston stat
DW统计量，用于检测误差是否序列相关，如果相关，可以通过预测误差，改进回归模型的效果。
值一般在[0, 4]之间，越接近2，说明不含自相关。
13. AIC
AIC准则用于预测模型的选择，越小越好
14. SIC
和AIC一样，用于预测模型的选择，同样是越小越好
~~~

**参考**
- https://www.csdn.net/article/2015-12-02/2826374
- https://blog.csdn.net/shine19930820/article/details/78335550
- https://lbxc.iteye.com/blog/1520649
- https://wiki.mbalib.com/wiki/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E6%B3%95
- https://zhuanlan.zhihu.com/p/36326966

# 逻辑回归的优缺点
~~~
逻辑回归应用到工业界当中一些优点：
1. 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
2. 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
3. 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
4. 资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。
5. 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。

但是逻辑回归本身也有许多的缺点:
1. 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
2. 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
3. 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
4. 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。
~~~

**参考**
- https://www.cnblogs.com/ModifyRong/p/7739955.html

# 样本不均衡问题解决办法
~~~
1. 扩大数据集
2. 尝试其它评价指标 
3. 对数据集进行重采样
4. 尝试产生人工数据样本 
5. 尝试不同的分类算法 
6. 尝试对模型进行惩罚 
7. 尝试一个新的角度理解问题 
8. 尝试创新 
~~~

**参考**
- https://blog.csdn.net/heyongluoyao8/article/details/49408131
- https://blog.csdn.net/login_sonata/article/details/54290402

# sklearn参数
~~~
1. 正则化选择参数（惩罚项的种类）
2. 优化算法选择参数
3. 分类方式选择参数
4. 类型权重参数：（考虑误分类代价敏感、分类类型不平衡的问题）
5. 样本权重参数
~~~

**参考**
- https://blog.csdn.net/CherDW/article/details/54891073
- https://blog.csdn.net/jark_/article/details/78342644
