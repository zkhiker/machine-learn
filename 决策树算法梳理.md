决策树算法梳理
=============

1. [信息论基础](#user-content-信息论基础)
~~~
熵、 联合熵、 条件熵、 信息增益、 基尼不纯度
~~~

2. [决策树的不同分类算法的原理及应用场景](#user-content-决策树的不同分类算法的原理及应用场景)
~~~
算法：ID3算法、C4.5、CART分类树
~~~

3. [回归树原理](#user-content-回归树原理)
4. [决策树防止过拟合手段](#user-content-决策树防止过拟合手段)
5. [模型评估](#user-content-模型评估)
6. [sklearn参数详解，Python绘制决策树](#user-content-sklearn参数详解，Python绘制决策树)

# 信息论基础

## 熵
~~~
在信息论中，熵（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。）来自信源的另一个特征是样本的概率分布。
~~~

**参考**
- http://home.ustc.edu.cn/~kunzhao/TA/ch1-2.pdf
- https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)
- https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E7%86%B5
- https://www.cnblogs.com/arachis/p/entropy.html
- https://zhuanlan.zhihu.com/p/36385989

## 联合熵
~~~
联合熵是一集变量之间不确定性的衡量手段
~~~

**参考**
- https://zh.wikipedia.org/wiki/%E8%81%94%E5%90%88%E7%86%B5

## 条件熵
~~~
在信息论中，条件熵描述了在已知第二个随机变量 {\displaystyle X} X 的值的前提下，随机变量 {\displaystyle Y} Y 的信息熵还有多少。同其它的信息熵一样，条件熵也用Sh、nat、Hart等信息单位表示。基于 {\displaystyle X} X 条件的 {\displaystyle Y} Y 的信息熵，用 {\displaystyle \mathrm {H} (Y|X)} \mathrm{H} (Y|X) 表示。
~~~

**参考**
- https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E7%86%B5
- https://zhuanlan.zhihu.com/p/26551798

## 信息增益
~~~
在概率论和信息论中，信息增益是非对称的，用以度量两种概率分布P和Q的差异。信息增益描述了当使用Q进行编码时，再使用P进行编码的差异。通常P代表样本或观察值的分布，也有可能是精确计算的理论分布。Q代表一种理论，模型，描述或者对P的近似。
尽管信息增益通常被直观地作为是一种度量或距离，但事实上信息增益并不是。就比如信息增益不是对称的，从P到Q的信息增益通常不等于从Q到P的信息增益。信息增益是f增益（f-divergences）的一种特殊情况。在1951年由Solomon Kullback 和Richard Leibler首先提出作为两个分布的直接增益（directed divergence）。它与微积分中的增益不同，但可以从Bregman增益（Bregman divergence）推导得到。
~~~

**参考**
- https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A
- https://zhuanlan.zhihu.com/p/26596036
- https://www.zhihu.com/question/22104055
- https://www.cnblogs.com/wentingtu/archive/2012/03/24/2416235.html

## 基尼不纯度
~~~
基尼不纯度是用于决策树编程中的一个专业术语。
基尼不纯度,是指将来自集合中的某种结果随机应用在集合中，某一数据项的预期误差率。
是在进行决策树编程的时候，对于混杂程度的预测中，一种度量方式。
~~~

**参考**
- https://baike.baidu.com/item/%E5%9F%BA%E5%B0%BC%E4%B8%8D%E7%BA%AF%E5%BA%A6/22046808?noadapt=1
- https://www.jianshu.com/p/661808626a66
- https://blog.csdn.net/lingtianyulong/article/details/34522757
- https://blog.csdn.net/JJBOOM425/article/details/79997440

# 决策树的不同分类算法的原理及应用场景

## 决策树分类算法
~~~
决策树学习使用一个决策树作为一个预测模型，它将对一个 item（表征在分支上）观察所得映射成关于该 item 的目标值的结论（表征在叶子中）。
树模型中的目标是可变的，可以采一组有限值，被称为分类树；在这些树结构中，叶子表示类标签，分支表示表征这些类标签的连接的特征。
算法例子：
1. 分类和回归树（Classification and Regression Tree，CART）
2. Iterative Dichotomiser 3（ID3）
3. C4.5 和 C5.0（一种强大方法的两个不同版本）
优点：
1. 容易解释
2. 非参数型
缺点：
1. 趋向过拟合
2. 可能或陷于局部最小值中
3. 没有在线学习
~~~

**参考**
- https://scholar.google.com.hk/scholar?q=%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E4%B8%8D%E5%90%8C%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF&hl=zh-CN&as_sdt=0&as_vis=1&oi=scholart
- https://www.ibm.com/developerworks/cn/analytics/library/ba-1507-decisiontree-algorithm/index.html
- https://blog.csdn.net/SEUer_jeff/article/details/65629877

## 原理和场景
~~~
// TODO 
~~~

**参考**

# 回归树原理
~~~
它采用一种二分递归分割的技术，分割方法采用基于最小距离的基尼指数估计函数，将当前的样本集分为两个子样本集，使得生成的的每个非叶子节点都有两个分支。因此，CART算法生成的决策树是结构简洁的二叉树。
~~~

**参考**
- https://juejin.im/post/5a7eb1f06fb9a0636108710a
- https://blog.csdn.net/u014568921/article/details/45082197
- https://blog.csdn.net/jiede1/article/details/76034328
- https://www.cnblogs.com/qwj-sysu/p/5974421.html
- https://www.cnblogs.com/qwj-sysu/p/5993939.html

# 决策树防止过拟合手段

| 过拟合问题          | 防止手段                                                      |
|--------------------|--------------------------------------------------------------|
| 样本问题            | 合理、有效地抽样，用相对能够反映业务逻辑的训练集去产生决策树      |
| 构建决策树的方法问题 | 剪枝：提前停止树的增长或者对已经生成的树按照一定的规则进行后剪枝。 |

~~~
剪枝的方法:
 (1) 先剪枝（prepruning）
 (2) 后剪枝（postpruning）
~~~

**参考**
- https://blog.csdn.net/sinat_32043495/article/details/78729610
- https://blog.csdn.net/qq_28168421/article/details/53456090
- https://blog.csdn.net/u014303046/article/details/53453177
- https://www.zhihu.com/question/59201590/answer/167392763
- https://www.jianshu.com/p/6e0485895701

# 模型评估
~~~
常用的评价指标有

错误率：分类错误的样本数占样本总数的比例。
准确率（精度）：分类正确的样本数占样本总数的比例。在很多情况下，准确率是一个欠佳或具有误导性的指标，比如在不同类型错误具有不同代价时（典型为分类不平衡，即正类别或负类别极其罕见）。
对数损失函数（置信度）
~~~

**参考**
- https://www.cnblogs.com/wkslearner/p/8748141.html
- https://feisky.xyz/machine-learning/basic/evaluation.html
- https://www.jianshu.com/p/41f434818ffc
- https://zhuanlan.zhihu.com/p/35792751
- https://blog.csdn.net/heyongluoyao8/article/details/49408319
- https://www.zybuluo.com/lyc102/note/714880

# sklearn参数详解，Python绘制决策树
~~~
scikit-learn决策树算法类库内部实现是使用了调优过的CART树算法，既可以做分类，又可以做回归。分类决策树的类对应的是DecisionTreeClassifier，而回归决策树的类对应的是DecisionTreeRegressor。两者的参数定义几乎完全相同，但是意义不全相同。下面就对DecisionTreeClassifier和DecisionTreeRegressor的重要参数做一个总结，重点比较两者参数使用的不同点和调参的注意点。
~~~

**参考**
- https://www.cnblogs.com/pinard/p/6056319.html
- https://blog.csdn.net/Yeoman92/article/details/73436632
